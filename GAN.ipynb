{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c35323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "#The dataset availability and their size is a huge issue in DL and ML. So, the code checks if the GAN generated dataset is fiseabile to be used as training datset.\n",
    "\n",
    "#The main purpose of this code is to check the efficiency of GAN generated artificial data in classification.\n",
    "#The GAN artificial dataset is generated using CGAN network.\n",
    "\n",
    "#The original MNIST-Fashion Dataset is first classified and the accuracy is obtained.\n",
    "#Next, the original dataset and the artificial dataset are combined in 50% proportions and tested again for classification using CNN.\n",
    "\n",
    "#The hyperparameters like epoch, sample_interval, batch_size, learning rate (alpha) are changed continuously and the current combination provides\n",
    "#with higher accuracy for 50% combined data, but it is an minimal increase.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import (\n",
    "        Activation, BatchNormalization, Concatenate, Dense, Dropout, Multiply,\n",
    "        Embedding, Flatten, Input, Reshape, LeakyReLU, Conv2D, Conv2DTranspose, MaxPooling2D) \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Define important parameters\n",
    "img_shape = (28,28,1)\n",
    "z_dim =100\n",
    "n_class= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c678944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(z_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256 * 7 * 7, input_dim=z_dim,))\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "\n",
    "    model.add(Conv2DTranspose(128, 3, 2, padding='same',))\n",
    "    model.add(LeakyReLU(alpha =0.01))\n",
    "\n",
    "    model.add(Conv2DTranspose(64, 3, 1, padding='same',))\n",
    "    model.add(LeakyReLU(alpha =0.01))\n",
    "\n",
    "    model.add(Conv2DTranspose(1, 3, 2, padding='same',))\n",
    "    model.add(LeakyReLU(alpha =0.01))\n",
    "\n",
    "    return model\n",
    "\n",
    "# generator input \n",
    "def generator(z_dim):\n",
    "    # latent input\n",
    "    z = Input(shape=(z_dim, ))\n",
    "    # label input\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "    # convert label to embedding\n",
    "    label_embedding = Embedding(n_class, z_dim)(label)\n",
    "\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    # dot product two inputs\n",
    "    joined_representation = Multiply()([z, label_embedding])\n",
    "\n",
    "    generator = generator_model(z_dim)\n",
    "\n",
    "    conditioned_img = generator(joined_representation)\n",
    "\n",
    "    model =  Model([z, label], conditioned_img)\n",
    "    # save model blueprint to image\n",
    "    plot_model(model,'generator.jpg',show_shapes=True,show_dtype=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aaf063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator CNN model\n",
    "def discriminator_model(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64,3,2,input_shape=(img_shape[0], img_shape[1], img_shape[2] + 1),))\n",
    "    model.add(LeakyReLU(alpha =0.01))\n",
    "\n",
    "    model.add(Conv2D(64,3,2,input_shape=img_shape,padding='same',))\n",
    "    model.add(LeakyReLU(alpha =0.001))\n",
    "\n",
    "    model.add(Conv2D(128,3,2,input_shape=img_shape,padding='same',))\n",
    "    model.add(LeakyReLU(alpha =0.001))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator(img_shape):\n",
    "    # image input\n",
    "    img = Input(shape=img_shape)\n",
    "    # label input\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "    label_embedding = Embedding(n_class, np.prod(img_shape),input_length=1)(label)\n",
    "\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "    label_embedding = Reshape(img_shape)(label_embedding)\n",
    "    # concatenate the image and label\n",
    "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "\n",
    "    discriminator = discriminator_model(img_shape)\n",
    "\n",
    "    classification = discriminator(concatenated)\n",
    "\n",
    "    model = Model([img, label], classification)\n",
    "\n",
    "    plot_model(model,'discriminator.jpg',show_shapes=True,show_dtype=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f72e24",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# define a complete GAN architecture\n",
    "def cgan(generator, discriminator):\n",
    "\n",
    "    z = Input(shape=(z_dim, ))\n",
    "\n",
    "    label = Input(shape=(1, ))\n",
    "\n",
    "    img = generator([z, label])\n",
    "\n",
    "    classification = discriminator([img, label])\n",
    "\n",
    "    model = Model([z, label], classification)\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = discriminator(img_shape)\n",
    "# compile the discriminator architecture \n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "generator = generator(z_dim)\n",
    "# set discriminator to non-trainanle \n",
    "discriminator.trainable = False\n",
    "# compile the whole C-GAN architectu\n",
    "cgan = cgan(generator, discriminator)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e32fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 38ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 36ms/step\n",
      "4/4 [==============================] - 0s 27ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 36ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 30ms/step\n",
      "4/4 [==============================] - 0s 38ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 37ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 59ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 30ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 37ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 29ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 38ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 36ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 36ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 30ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 38ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 30ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 40ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 30ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 38ms/step\n",
      "4/4 [==============================] - 0s 51ms/step\n",
      "4/4 [==============================] - 0s 38ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 41ms/step\n",
      "4/4 [==============================] - 0s 40ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n",
      "4/4 [==============================] - 0s 30ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 35ms/step\n",
      "4/4 [==============================] - 0s 31ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 32ms/step\n",
      "4/4 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "# label to category dictionary\n",
    "dict_clothes ={0: \"T-shirt/top\",1: \"Trouser\",2: \"Pullover\",\n",
    "               3: \"Dress\",4: \"Coat\",5: \"Sandal\",6: \"Shirt\",\n",
    "               7: \"Sneaker\",8: \"Bag\",9: \"Ankle boot\"}\n",
    "\n",
    "# function to plot and save sample images\n",
    "def plot_sample_images(epoch ,rows=5,columns=4):\n",
    "\n",
    "    z = np.random.normal(0, 1, (rows * columns, z_dim))\n",
    "    a =np.arange(0,10)\n",
    "    b =np.arange(0,10)\n",
    "\n",
    "    labels = np.append(a,b).reshape(-1,1)\n",
    "    \n",
    "    gen_imgs = generator.predict([z, labels])\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    print(\"Epoch : %d \"%(epoch+1))\n",
    "    fig, axs = plt.subplots(rows,\n",
    "                            columns,\n",
    "                            figsize =(50, 20),\n",
    "                            sharey=True,\n",
    "                            sharex=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            axs[i, j].set_title(\"Type: %s\" % dict_clothes.get(labels[cnt][0]))\n",
    "            cnt += 1\n",
    "    fig.savefig('image%d.jpg'%(epoch))\n",
    "\n",
    " \n",
    "# define training step\n",
    "def train(epochs, batch_size, sample_interval):\n",
    "    #  import Fashion-MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test)  = fashion_mnist.load_data()\n",
    "    X_train = np.concatenate([x_train, x_test])\n",
    "    Y_train = np.concatenate([y_train, y_test])\n",
    "\n",
    "    X_train = X_train.astype(\"float32\") / 255.0\n",
    "    X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
    "\n",
    "\n",
    "    real = np.ones((batch_size, 1))\n",
    "\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs, labels = X_train[idx], Y_train[idx]\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "        # generate images from generator\n",
    "        gen_imgs = generator.predict([z, labels])\n",
    "        # pass real an generated images to the discriminator and ctrain on them\n",
    "        d_loss_real = discriminator.train_on_batch([imgs, labels], real)\n",
    "        d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "\n",
    "        labels = np.random.randint(0, n_class, batch_size).reshape(-1, 1)\n",
    "   \n",
    "        g_loss = cgan.train_on_batch([z, labels], real)\n",
    "        \n",
    "        gen_imgs.to_csv('artificial data.csv',mode='a')\n",
    "        \n",
    "        if (epoch + 1) % sample_interval == 0:\n",
    "\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %(epoch + 1, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "            plot_sample_images(epoch+1)\n",
    "iterations = 2000\n",
    "batch_size = 128\n",
    "sample_interval = 2000\n",
    "\n",
    "train(iterations, batch_size, sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8f4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Training Dataset.\n",
    "x=pd.read_csv(r\"C:\\Users\\Surya\\Desktop\\MNIST\\fashion-mnist_train.csv\")\n",
    "\n",
    "#Labels.\n",
    "y_train=x['label']\n",
    "\n",
    "#Features.\n",
    "x_train=x.drop(['label'],axis=1)\n",
    "\n",
    "#Storing the Features, for further need.\n",
    "x_train0=x_train[2::2]\n",
    "y_train0=y_train[2::2]\n",
    "\n",
    "#Reshaping into a 3D numpy array to apply CNN.\n",
    "x_train=np.asarray(x_train)\n",
    "x_train=x_train.reshape([-1,28,28,1])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bf6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Testing Dataset.\n",
    "x=pd.read_csv(r\"C:\\Users\\Surya\\Desktop\\MNIST\\fashion-mnist_test.csv\")\n",
    "\n",
    "#Labels.\n",
    "y_test=x['label']\n",
    "\n",
    "#Features.\n",
    "x_test=x.drop(['label'],axis=1)\n",
    "\n",
    "#Reshaping into a 3D numpy array to apply CNN.\n",
    "x_test=np.asarray(x_test)\n",
    "x_test=x_test.reshape([-1,28,28,1])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29db5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model with relu activation functions and sigmoid at output layer.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bce3829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 10816)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                173072    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,882\n",
      "Trainable params: 173,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Complete model is now compiled and the description is shown.\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8589a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "118/118 [==============================] - 37s 304ms/step - loss: 0.7222 - accuracy: 0.7588 - val_loss: 0.4579 - val_accuracy: 0.8412\n",
      "Epoch 2/5\n",
      "118/118 [==============================] - 35s 298ms/step - loss: 0.4435 - accuracy: 0.8420 - val_loss: 0.3945 - val_accuracy: 0.8641\n",
      "Epoch 3/5\n",
      "118/118 [==============================] - 35s 295ms/step - loss: 0.3930 - accuracy: 0.8620 - val_loss: 0.3615 - val_accuracy: 0.8777\n",
      "Epoch 4/5\n",
      "118/118 [==============================] - 35s 294ms/step - loss: 0.3629 - accuracy: 0.8727 - val_loss: 0.3527 - val_accuracy: 0.8775\n",
      "Epoch 5/5\n",
      "118/118 [==============================] - 35s 301ms/step - loss: 0.3459 - accuracy: 0.8788 - val_loss: 0.3171 - val_accuracy: 0.8925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212e0864e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model with the original MNIST-Fashion Dataset using CNN.\n",
    "model.fit(x=x_train, y=y_train, batch_size=512, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f4f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.3171 - accuracy: 0.8925\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of original MNIST-Fashion Dataset using CNN.\n",
    "accuracy=model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3bca3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The artificial dataset generated using GAN.\n",
    "x=pd.read_csv(r\"C:\\Users\\Surya\\Desktop\\MNIST\\artificial data.csv\")\n",
    "\n",
    "#Labels are available as CGAN is implemented. \n",
    "y_train1=x['label']\n",
    "\n",
    "#Features.\n",
    "x_train1=x.drop(['label'],axis=1)\n",
    "\n",
    "#Reshaping.\n",
    "x_train1=np.asarray(x_train1)\n",
    "x_train1=x_train1.reshape([-1,28,28,1])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c47ef45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half of the actual MNIST- Fashion Dataset.\n",
    "x_train1=x_train0\n",
    "y_train1=y_train0\n",
    "\n",
    "#New dataset with 50% original and 50% CGAN generated is used.\n",
    "x_train=np.concatenate((x_train1,x_train0),axis=0)\n",
    "y_train=np.concatenate((y_train1,y_train0),axis=0)\n",
    "\n",
    "x_train=x_train.astype(float)\n",
    "y_train=y_train.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d09e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "235/235 [==============================] - 72s 302ms/step - loss: 0.3270 - accuracy: 0.8842 - val_loss: 0.2973 - val_accuracy: 0.8990\n",
      "Epoch 2/5\n",
      "235/235 [==============================] - 74s 316ms/step - loss: 0.3050 - accuracy: 0.8927 - val_loss: 0.2887 - val_accuracy: 0.8980\n",
      "Epoch 3/5\n",
      "235/235 [==============================] - 70s 296ms/step - loss: 0.2893 - accuracy: 0.8974 - val_loss: 0.2796 - val_accuracy: 0.9029\n",
      "Epoch 4/5\n",
      "235/235 [==============================] - 69s 292ms/step - loss: 0.2774 - accuracy: 0.9011 - val_loss: 0.2683 - val_accuracy: 0.9065\n",
      "Epoch 5/5\n",
      "235/235 [==============================] - 69s 295ms/step - loss: 0.2697 - accuracy: 0.9040 - val_loss: 0.2601 - val_accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212e6906110>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model.\n",
    "model.fit(x=x_train, y=y_train, batch_size=512, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5931ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2601 - accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "#Accuracy.\n",
    "accuracy=model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
